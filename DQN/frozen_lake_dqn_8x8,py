import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import random
import torch
from torch import nn
import torch.nn.functional as F

# Define model
class DQN(nn.Module):
    def __init__(self, in_states, h1_nodes, out_actions):
        super().__init__()
        self.fc1 = nn.Linear(in_states, h1_nodes) # first layer: 64 nodes
        self.fc2 = nn.Linear(h1_nodes, h1_nodes)  # second hidden layer: 128 nodes
        self.out = nn.Linear(h1_nodes, out_actions) # output layer

    def forward(self, x):
        x = F.relu(self.fc1(x))  # apply RELU to the first layer
        x = F.relu(self.fc2(x))  # apply RELU to the second layer
        x = self.out(x) # calculate output
        return x

# Define memory for Experience Replay
class ReplayMemory():
    def __init__(self, maxlen):
        self.memory = deque([], maxlen=maxlen)
    
    def append(self, transition):
        self.memory.append(transition)

    def sample(self, sample_size):
        return random.sample(self.memory, sample_size)

    def __len__(self):
        return len(self.memory)

# FrozenLake Deep Q-Learning (8x8 version)
class FrozenLakeDQL():
    # Hyperparameters (adjusted for 8x8)
    learning_rate_a = 0.001         # Lower learning rate for stability
    discount_factor_g = 0.9        # Slightly higher discount factor for larger map
    network_sync_rate = 500         # Sync target network less frequently
    replay_memory_size = 10000      # Larger replay memory
    mini_batch_size = 128           # Larger batch size
    epsilon_decay = 0.9995          # Slower epsilon decay

    # Neural Network
    loss_fn = nn.MSELoss() # NN loss function, MSE chosen
    optimizer = None # NN optimizer, initialize later.
    ACTIONS = ['L', 'D', 'R', 'U']  # Actions: Left, Down, Right, Up

    # Train the FrozenLake environment
    def train(self, episodes, render=False, is_slippery=False):
        env = gym.make('FrozenLake8x8-v1', is_slippery=is_slippery, render_mode='human' if render else None)
        num_states = env.observation_space.n  # 8x8 => 64 states
        num_actions = env.action_space.n      # 4 actions

        epsilon = 1.0
        memory = ReplayMemory(self.replay_memory_size)

        # creation of policy and target network, and copy of the policy one in the target network
        policy_dqn = DQN(in_states=num_states, h1_nodes=128, out_actions=num_actions)
        target_dqn = DQN(in_states=num_states, h1_nodes=128, out_actions=num_actions)
        target_dqn.load_state_dict(policy_dqn.state_dict())

        print('Policy (random, before training):')
        self.print_dqn(policy_dqn, map_size=8)  # Print 8x8 grid
        # Adam optimizer chosen
        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)
        # list to keep track of rewards collected per episode. Initialize list to 0's.
        rewards_per_episode = np.zeros(episodes)
        # list to keep track of epsilon decay
        epsilon_history = []
        # track number of steps taken. Used for syncing policy => target network.
        step_count = 0

        for i in range(episodes):
            state = env.reset()[0]
            terminated = False
            truncated = False
            
            while not terminated and not truncated:
                # selection based on epsilon greedy policy
                if random.random() < epsilon:
                    # select random action
                    action = env.action_space.sample()
                else:
                    # select best action
                    with torch.no_grad():
                        action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()

                new_state, reward, terminated, truncated, _ = env.step(action) # execute action
                memory.append((state, action, new_state, reward, terminated)) # save experience in the memory
                state = new_state # move to the next state
                step_count += 1 # increment the step counter

            if reward == 1:
                rewards_per_episode[i] = 1

            # Train if enough samples in memory
            if len(memory) >= self.mini_batch_size:
                mini_batch = memory.sample(self.mini_batch_size)
                self.optimize(mini_batch, policy_dqn, target_dqn)

                # Decay epsilon
                epsilon = max(epsilon * self.epsilon_decay, 0.01)
                epsilon_history.append(epsilon)

                # Sync target network
                if step_count >= self.network_sync_rate:
                    target_dqn.load_state_dict(policy_dqn.state_dict())
                    step_count = 0

            # Print progress
            if (i + 1) % 100 == 0:
                print(f"Episode: {i+1}, Epsilon: {epsilon:.3f}, Success Rate: {np.mean(rewards_per_episode[max(0, i-100):i+1]):.2f}")

        env.close()
        torch.save(policy_dqn.state_dict(), "frozen_lake8x8_dql.pt")

        # Plotting
        plt.figure(figsize=(12, 5))
        plt.subplot(1, 2, 1)
        plt.plot(np.convolve(rewards_per_episode, np.ones(100)/100, mode='valid'))
        plt.title("Rewards per Episode (Smoothed)")
        plt.subplot(1, 2, 2)
        plt.plot(epsilon_history)
        plt.title("Epsilon Decay")
        plt.savefig('frozen_lake8x8_training.png')

    # Optimization function (same as before)
    def optimize(self, mini_batch, policy_dqn, target_dqn):
        num_states = policy_dqn.fc1.in_features
        current_q_list = []
        target_q_list = []

        for state, action, new_state, reward, terminated in mini_batch:
            if terminated:
                target = torch.FloatTensor([reward])
            else:
                with torch.no_grad():
                    target = torch.FloatTensor(
                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()
                    )

            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))
            target_q = target_dqn(self.state_to_dqn_input(state, num_states))
            target_q[action] = target

            current_q_list.append(current_q)
            target_q_list.append(target_q)

        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    # Convert state to one-hot tensor
    def state_to_dqn_input(self, state: int, num_states: int) -> torch.Tensor:
        return torch.FloatTensor([1 if i == state else 0 for i in range(num_states)])

    # Test the trained policy
    def test(self, episodes, is_slippery=False):
        env = gym.make('FrozenLake8x8-v1', is_slippery=is_slippery, render_mode='human')
        num_states = env.observation_space.n
        num_actions = env.action_space.n

        policy_dqn = DQN(in_states=num_states, h1_nodes=128, out_actions=num_actions)
        policy_dqn.load_state_dict(torch.load("frozen_lake8x8_dql.pt"))
        policy_dqn.eval()

        print('Trained Policy (8x8):')
        self.print_dqn(policy_dqn, map_size=8)

        successes = 0
        for _ in range(episodes):
            state = env.reset()[0]
            terminated = False
            truncated = False

            while not terminated and not truncated:
                with torch.no_grad():
                    action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()
                state, reward, terminated, truncated, _ = env.step(action)

            if reward == 1:
                successes += 1

        print(f"Success Rate: {successes / episodes * 100:.2f}%")
        env.close()

    # Print policy (adjusted for 8x8 grid)
    def print_dqn(self, dqn, map_size=8):
        num_states = dqn.fc1.in_features
        for s in range(num_states):
            q_values = ' '.join([f"{q:.2f}" for q in dqn(self.state_to_dqn_input(s, num_states)).tolist()])
            best_action = self.ACTIONS[dqn(self.state_to_dqn_input(s, num_states)).argmax()]
            print(f'{s:02},{best_action},[{q_values}]', end=' ')
            if (s + 1) % map_size == 0:
                print()

if __name__ == '__main__':
    frozen_lake = FrozenLakeDQL()
    is_slippery = True  # Set to False for deterministic version
    frozen_lake.train(5000, is_slippery=is_slippery)  # Train for more episodes
    frozen_lake.test(20, is_slippery=is_slippery)  # Test 20 episodes